---
title: Zombie links on data catalogs
description: Some of these links were less dead than I'd thought.
tweet_text: Are they dead, or are they alive? http://thomaslevine.com/!/data-catalog-dead-links #opendata
twitter_image: figure/prop_links.png
facebook_image: figure/prop_links.png
tags: ['open-data']
kind: article
created_at: 2014-01-28
---
```{r configure, echo=FALSE}
opts_chunk$set(echo = FALSE, dpi = 42 * 5)
```
After I wrote about
[dead links on data catalogs](/!/dead-links-on-data-catalogs),
some people commented that the links were less dead than I'd thought.

[![Anecdotally I don't think that @DatiTrentinoit has so many broken links. Check validator? @thomaslevine http://thomaslevine.com/!/data-catalog-dead-links/](trentino.png)](https://twitter.com/statshero/status/424147773852622848)

[![@thomaslevine You've got ~45% of data․openva․com datasets missing. I just audited 75% of them, and found just 2 missing. Any idea what's up?](openva.png)](https://twitter.com/waldojaquith/status/424026174508261376)

Some explanations were proposed. Samuele and Jindřich both suggested that
the CKAN FileStore doesn't support HEAD requests.

[![@DatiTrentinoit @statshero @thomaslevine Ckan returns 404 on HEAD requests, that's the problem..](samuele.png)])https://twitter.com/_rshk/status/424208140016418816)

[![](jindrich.png)](https://twitter.com/jindrichmynarz/status/428194318063370241)

And Waldo suggested that I might be checking for status code 200 but
receiving status code 303 (redirect) from OpenVA.

So what was going on?

## Not reasons
I considered the two possibilities that were mentioned above, and
I don't think either of them was the issue.

### HEAD
CKAN does just fine on HEAD requests.

  url = 'http://dati.trentino.it/storage/f/2013-06-16T114537/_EBmYVk.csv'
  import requests

  get = requests.get(url)
  head = requests.head(url)
  
  print(get.status_code)
  # 200
  print(head)
  # 200

It could be that there was a CKAN bug and it has been fixed in the past two
weeks, but I doubt it.

I don't think the issue was that CKAN returns 404 on HEAD requests.

### Status code

XXX

## Reasons
If the HEAD request thing and the redirect status code weren't the issues,
what was?

To figure this out, I tweaked my downloader and ran it on just the links that
had timed out or otherwise not responded; I didn't run it on links that had
responded with error status codes like 404. I also pulled out the hostname of
the links. (For example, `thomaslevine.com` of the URL of the page you're
reading.) Then I looked at the errors I got back.

I also looked around in the SQLite3 database in which I'd been storing
everything for this link analysis.

It's not like there was just one issue, of course.
Here are the main factors I see as leading to the strange results.

* I didn't fully parse links to datasets.
* I had a low timeout on my HTTP requests (2 seconds).
* I had duplicate data in my database.

### Incompletely parsed URLs
I looked at the different sorts of errors that I got when I requested links
to different hostnames.

```{r p_hostname_error}
p.hostname.error
```

That the left-most bar, for the hostname `http:`, is quite large and has
a lot of invalid URLs. Things that my hostname-parser detected as `http:` are
usually invalid URLs.

```{r invalid_urls}
subset(errors, error_type == 'InvalidURL')[1:5,'url']
```

### Low Timeout
Aside from the invalid URLs, most of the links gave no longer or gave a timeout.

```{r p_errors_total}
p.errors.total
```

As we saw above, different websites (hostnames) tend to give different errors.
The following plot should make it more clear.

```{r p_hostname_facet_again}
p.hostname.facet
```

The `/storage` endpoint is typically used for the CKAN filestore. It looks like
that can take a while to respond.

```{r storage}
p.storage
```

## Duplicate data
It turned out that I had duplicate records in my table of link information. As I was working
with it, I forgot the correct schema and remembered a different one; I thought I had a unique
index on something for which I didn't.

```{r p_duplicates_ckan}
p.duplicates.ckan
```

```{r p_duplicates_socrata}
p.duplicates.socrata
```

XXX Write a sentence after looking at the plots.

## New results
I used a longer timeout on the previously-erring links and deduplicated my duplicates.
Here are the new results.

XXX Write a sentence after looking at the plot.

```{r p_link_types}
p.link.types
```

XXX Write a sentence after looking at the plot.

```{r p_link_types_onlylinks}
p.link.types.onlylinks
```

XXX Write a sentence after looking at the plot.

```{r p_link_types_specifics}
p.link.types.specifics
```

XXX Write a sentence after looking at the plot.

```{r p_prop_links}
p.prop_links
```

XXX Write a sentence after looking at the plot.

# Appendix: How I figured this all out

## Status codes
I called a URL alive if an ordinary HEAD request to it returned a
[status code](http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html) of 200.
This simplifies things a little bit.
I started out by considering whether this was an appropriate test.

Here are all of the status codes that I received, from all of the different
links from all of the catalogs.

```{r status_codes}
p.codes
```

Let's look at this for just the specific catalogs that those Tweets were about.


```{r status_codes_trentino_va}
```

```r
sort(table(subset(datasets, catalog == 'data.openva.com')$status_code), decreasing = T)[1:5]
```

```r
sort(table(subset(datasets, catalog == 'dati.trentino.it')$status_code), decreasing = T)[1:5]
```

Aside from 200, all of those status codes are errors, so this method of checking
seems fine. On the other hand, it seems like there were a lot of non-responses....
More about non-responses later; for now, we'll just say that the status code
check is fine.

## Duplicates
When plotting this, I realized that some of the data didn't line up. I set up
a database schema that was more normalized so that I wouldn't check a link twice
if two datasets linked to it. I thus had a datasets table and a links table.

Then I wound up changing my mind and using the links table to store a single record
per dataset. Here arose a problem; there was no unique index on this datasets table,
but I thought there was, so I added multiple records for each link.

```{r duplicates}
kable(sqldf("
SELECT catalog, identifier, count(*)
FROM links
WHERE software = 'ckan'
GROUP BY catalog, identifier
ORDER BY count(*) DESC
LIMIT 10;
", dbname = '/tmp/open-data.sqlite'))
```

I had thought that there was a unique index on
`links.software, links.catalog, links.identifier`,
but there wasn't!


## Misinterpreting NULLs
Another issue: I had interpreted NULL as meaning that the dataset is not a link,
but it really represents that link's liveliness has yet  to be checked;
here's the relevant line of code.

    url_list = [row['url'] for row in dt.execute('SELECT DISTINCT url FROM links WHERE status_code IS NULL')]

I don't remember exactly how this affected the results thought; it might not have
been a big deal.

## Unresponsive datasets
I recorded when HTTP requests for datasets had timed out or otherwise
not responded. (In the database, these are indicated as status code -42.)
I checked a few of these manually. Here are two of them.

    dati.trentino.it/storage/f/2013-06-16T111814/_ggeiWE.csv
    https://www-genesis.destatis.de/genesis/online/link/tabelleDownload/46421-0001.html

They both work, but they take a while. Also, I found that they took longer
to download to my desktop computer on my desk than they took to download to
my server in a datacenter.
In case the problem was my internet connection (tethered from a phone),
I used that server in a datacenter to run the checker again on
all datasets that had timed out. Results didn't remarkably change.

## Slow datasets
Those links took a while to download. Maybe my timeout threshold is being hit?

  url = 'http://dati.trentino.it/storage/f/2013-06-16T114537/_EBmYVk.csv'
  import requests

  timeout = requests.head(url, timeout = 2)
  # timeout error

So it is. More detail follows.

  get = requests.get(url)
  head = requests.head(url)

  print(get)
  # <Response [200]>

  print(head)
  # <Response [200]>

  print(head.elapsed)
  # datetime.timedelta(0, 3, 353558)

The link is alive, but my timeout of 2 seconds was too short.
In this case, the request with a timeout failed, so the initial
response took too long. Also, it took a total of 3.35 seconds to
download. The elapsed time isn't the same thing as the time until
which the request will time out, but they're related.

### Bad URLs
A bunch of datasets have a field for an external link but provided
an empty URL.

```{r bad_urls}
sqldf("
SELECT catalog,count(*)
FROM links
WHERE is_link AND url = ''
GROUP BY catalog
ORDER BY 2;
", dbname = '/tmp/open-data.sqlite')
```

Aside from being interesting in itself, this pointed out to me that
there were probably lots of types of errors that I hadn't really
thought about. I realized I could look at the errors for each of the links
I checked. I thought I had saved the error in the database, but
my code for doing that turned out to be broken.

So I fixed that! Collecting better error information, I ran the checker
on all of the links that had failed before.

### Hostname
I figured that errors might be related to the server that a dataset comes
from, and I figured that the hostname of the URL would be a decent proxy
for server. (In the URL "http://thomaslevine.com/open-data", the hostname
is "thomaslevine.com".) So I wrote a sloppy function to detect these hostnames.

      datasets$hostname <- sub('(?:(?:http|ftp|https)://)?([^/]*)/.*$', '\\1', datasets$url)

Here are the top few hostnames and the number of datasets with each hostname.

```{r hostnames}
sort(table(datasets$hostname))[1:10]
```

Having come up with this variable, I now could look at error types by hostname.

### Base error rate
```{r p_errors_total}
p.errors.total
```

```{r p_hostname_total}
p.hostname.total
```

```{r p_hostname_error}
p.hostname.error
```

```{r p_hostname_facet}
p.hostname.facet
```

### Invalid URLs
The "http:" datasets weren't valid URLs.

```{r invalid_urls}
t.hostnames[['http:']]
```

### Connection errors
Connection errors seem to correspond to some datasets with strange URLs and others for
which the site just can't be contacted.

```{r connectionerror}
errors.connectionerror <- subset(errors, error_type == 'ConnectionError')
errors.connectionerror$has.tab <- grepl('\t', errors.connectionerror$url)
errors.connectionerror$has.space <- grepl(' ', errors.connectionerror$url)
errors.connectionerror$has.backslash <- grepl('[\\]', errors.connectionerror$url)
ggplot(errors.connectionerror) +
  aes(x = has.tab | has.space | has.backslash) +
  geom_bar()
```

### Invalid schemas
Invalid schemas are for datasets sent over protocals other than HTTP, like FTP.

```{r invalidschema}
ggplot(subset(errors, error_type == 'InvalidSchema')) + aes(x = sub(':.*', '', url)) + geom_bar()
```

Most of these schemas indicate that the files are stored on local systems
rather than being accessible from the internet. But a large minority of these
(FTP, specifically) is fully reasonable to put on the internet; I didn't
check them properly.

### Missing schemas
Missing schemas tend to be for datasets where the hostname was not specified.
Examples:

```{r table_missing_schema}
t.error_types$MissingSchema$url
```

```{r missingschema}
ggplot(subset(errors, error_type == 'MissingSchema')) + aes(x = grepl('^/', url)) + geom_bar()
```

This is a valid relative URL. I could have gotten the hostname from the site
from which I got the link, but I did not do this.

### SSL Errors
A bunch of sites did not have SSL certificates that I recognized.

```{r p_ssl_error}
ggplot(subset(errors, error_type == 'SSLError')) +
  aes(x = hostname) + geom_bar() + coord_flip() +
  ylab('Number of datasets with SSL errors')
```

I could ignore the certificates and download the dataset, but the SSL warning
is slightly unnerving.

SSL errors explain only a small part of the links I marked as dead. Of the
datasets that I'd marked as dead before, here are the numbers of datasets
with and without SSL errors.

```{r t_ssl}
table(errors$error_type == 'SSLError')
```

They are interesting, but they don't explain my strange results.

### Timeouts
Once we get rid of the strange URLs, most of these links have no errors or have
timeouts. (Remember, these are the links that I marked as dead in my previous
analysis, and I tried downloading again for the present analysis.)

```{r p_hostname_facet_again}
p.hostname.facet
```

The "www-genesis.destatis.de" datasets seem mostly okay, though there are some timeouts.

```{r plot_destatis}
ggplot(subset(errors, hostname == 'www-genesis.destatis.de')) + aes(x = error_type) + geom_bar()
```

```{r table_destatis}
t.hostnames[['www-genesis.destatis.de']]
```

```{r p_elapsed}
p.elapsed
```

```{r errors_elapsed}
errors.elapsed
```

The `/storage` endpoint is typically used for the CKAN filestore. It looks like
that can take a while to respond.

```{r storage}
p.storage
```
