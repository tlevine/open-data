---
title: Zombie links on data catalogs
description: Some of these links were less dead than I'd thought.
tweet_text: Are they dead, or are they alive? http://thomaslevine.com/!/data-catalog-dead-links #opendata
twitter_image: figure/prop_links.png
facebook_image: figure/prop_links.png
tags: ['open-data']
kind: article
created_at: 2014-01-28
---
```{r configure, echo=FALSE}
opts_chunk$set(echo = FALSE, dpi = 42 * 5)
```
After I wrote about
[dead links on data catalogs](/!/dead-links-on-data-catalogs),
some people commented that the links were less dead than I'd thought.


Some explanations were proposed.

**CKAN fails on HEAD requests**

**Redirects (Waldo)**

What was really going on?

## Status codes
I called a URL alive if an ordinary HEAD request to it returned a
[status code](http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html) of 200.
This simplifies things a little bit.

Here are all of the status codes that I received, from all of the different
links from all of the catalogs.

```{r status_codes}
p.codes
```

Let's look at this plot for just the specific catalogs that those Tweets were
about.




It seems like there were a lot of non-responses,

```r
sort(table(subset(datasets, catalog == 'data.openva.com')$status_code), decreasing = T)[1:5]
```

```r
sort(table(subset(datasets, catalog == 'dati.trentino.it')$status_code), decreasing = T)[1:5]
```

but that doesn't fully explain the results in the chart from the other post.
It looks like I have duplicate records in the table.

    select catalog, identifier, count(*) from links where software = 'ckan' group by catalog, identifier order by count(*) desc limit 1;



I had thought that there was a unique index on
`links.software, links.catalog, links.identifier`,
but there wasn't!



also, I had interpreted NULL as meaning that the dataset is not a link,
but it really represents that link's liveliness has yet  to be checked.

    url_list = [row['url'] for row in dt.execute('SELECT DISTINCT url FROM links WHERE status_code IS NULL')]


Here are some non-link datasets in the links table.

    select 'https://' || links.catalog || '/d/' || id url from links join socrata where not is_link and links.identifier=socrata.tableId limit 10;                                                                         
    url                               
    ----------------------------------
    https://data.sfgov.org/d/3fig-nit3
    https://data.sfgov.org/d/3hay-yzem
    https://data.sfgov.org/d/3nwz-3n68
    https://data.sfgov.org/d/3twj-ueew
    https://data.sfgov.org/d/4ang-frd3
    https://data.sfgov.org/d/5q3n-q6kw
    https://data.sfgov.org/d/7ybj-xpju
    https://data.sfgov.org/d/99js-dqmz
    https://data.sfgov.org/d/akvp-jmwa
    https://data.sfgov.org/d/di4e-7emh


I ran again things that timed out.
I checked a few manually.

dati.trentino.it/storage/f/2013-06-16T111814/_ggeiWE.csv

This works but sometimes takes a while.

https://www-genesis.destatis.de/genesis/online/link/tabelleDownload/46421-0001.html

In case the problem was an internet connection (tethered from a phone),
I used a big-boy computer in a datacenter to run the checker again on
all datasets that had timed out. Results didn't remarkably change.





## Slow datasets
Now I think this is what was going on.

  url = 'http://dati.trentino.it/storage/f/2013-06-16T114537/_EBmYVk.csv'
  import requests

  get = requests.get(url)
  head = requests.head(url)                                                                                     

  print(get)
  # <Response [200]>

  print(head)
  # <Response [200]>

  print(head.elapsed)
  # datetime.timedelta(0, 3, 353558)

The link is alive, but my timeout of 4 seconds (originally 2 seconds)
might be too short. It would have worked in this case, but it came kind
of close.











A bunch of datasets have listed links but provided an empty URL.

    select catalog,count(*) from links where is_link and url is null group by catalog order by 2;



sqlite> select count(*) from links where url like '//%';
0



I didn't have good error logs, so I fixed that.




### Inspecting data from each hostname

#### Invalid URLs

The "http:" datasets weren't valid URLs.

```{r}
t.hostnames[['http:']]
```

#### More invalid URLs
It turns out that missing schemas, invalid schemas,

connection errors and SSL errors
could all be 

Connection errors seem to correspond to some datasets with strange URLs and others for
which the site just can't be contacted.

errors.connectionerror <- subset(errors, error_type == 'ConnectionError')
errors.connectionerror$has.tab <- grepl('\t', errors.connectionerror$url)
errors.connectionerror$has.space <- grepl(' ', errors.connectionerror$url)
errors.connectionerror$has.backslash <- grepl('[\\]', errors.connectionerror$url)
ggplot(errors.connectionerror) +
  aes(x = has.tab | has.space | has.backslash) +
  geom_bar()

Invalid schemas are for datasets sent over protocals other than HTTP, like FTP.
Most of these schemas indicate that the files are stored on local systems
rather than being accessible from the internet.

ggplot(subset(errors, error_type == 'InvalidSchema')) + aes(x = sub(':.*', '', url)) + geom_bar()


Missing schemas are for datasets where the hostname was not specified. For example

t.error_types$MissingSchema$url

ggplot(subset(errors, error_type == 'MissingSchema')) + aes(x = grepl('^/', url)) + geom_bar()




#### Timeouts
Once we get rid of the invalid URLs, most of these links have no errors or have
timeouts. (Remember, these are the links that I marked as dead in my previous
analysis, and I tried downloading again for the present analysis.)

p.hostname.facet

The "www-genesis.destatis.de" datasets seem mostly okay, though there are some timeouts.

  ggplot(subset(errors, hostname == 'www-genesis.destatis.de')) + aes(x = error_type) + geom_bar()


  t.hostnames[['www-genesis.destatis.de']]

