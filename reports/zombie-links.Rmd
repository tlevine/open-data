---
title: Zombie links on data catalogs
description: Some of these links were less dead than I'd thought.
tweet_text: Are they dead, or are they alive? http://thomaslevine.com/!/data-catalog-dead-links #opendata
twitter_image: figure/prop_links.png
facebook_image: figure/prop_links.png
tags: ['open-data']
kind: article
created_at: 2014-01-28
---
```{r configure, echo=FALSE}
opts_chunk$set(echo = FALSE, dpi = 42 * 5)
```
After I wrote about
[dead links on data catalogs](/!/dead-links-on-data-catalogs),
some people commented that the links were less dead than I'd thought.

**trentino*

**openva**

Some explanations were proposed.

**CKAN fails on HEAD requests**

**Redirects (Waldo)**

It turns out that neither of these were the issue. Also, there wasn't just
one issue. Here are the main factor I identified as leading to the strange
results.

* I didn't fully parse links to datasets.
* I had a low timeout on my HTTP requests (2 seconds).
* I had duplicate data in my database.

## Parsing links

```{r}
p.scheme
```

```{r}
p.scheme.count
```

```{r}
p.scheme.prop
```

```{r}
p.no_scheme
```

## Low Timeout

## Duplicate data

```{r}
p.duplicates.ckan
```

```{r}
p.duplicates.socrata
```

## New results

```{r}
p.link.types
```

```{r}
p.link.types.onlylinks
```

```{r}
p.link.types.specifics
```

```{r}
p.prop_links
```

# Appendix: How I figured this all out

## Status codes
I called a URL alive if an ordinary HEAD request to it returned a
[status code](http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html) of 200.
This simplifies things a little bit.
I started out by considering whether this was an appropriate test.

Here are all of the status codes that I received, from all of the different
links from all of the catalogs.

```{r status_codes}
p.codes
```

Let's look at this for just the specific catalogs that those Tweets were about.


```{r status_codes_trentino_va}
```

```r
sort(table(subset(datasets, catalog == 'data.openva.com')$status_code), decreasing = T)[1:5]
```

```r
sort(table(subset(datasets, catalog == 'dati.trentino.it')$status_code), decreasing = T)[1:5]
```

Aside from 200, all of those status codes are errors, so this method of checking
seems fine. On the other hand, it seems like there were a lot of non-responses....
More about non-responses later; for now, we'll just say that the status code
check is fine.

## Duplicates
When plotting this, I realized that some of the data didn't line up. I set up
a database schema that was more normalized so that I wouldn't check a link twice
if two datasets linked to it. I thus had a datasets table and a links table.

Then I wound up changing my mind and using the links table to store a single record
per dataset. Here arose a problem; there was no unique index on this datasets table,
but I thought there was, so I added multiple records for each link.

```{r}
sqldf("
SELECT catalog, identifier, count(*)
FROM links
WHERE software = 'ckan'
GROUP BY catalog, identifier
ORDER BY count(*) DESC
LIMIT 10;
", dbname = '/tmp/open-data.sqlite')
```

I had thought that there was a unique index on
`links.software, links.catalog, links.identifier`,
but there wasn't!


## Misinterpreting NULLs
Another issue: I had interpreted NULL as meaning that the dataset is not a link,
but it really represents that link's liveliness has yet  to be checked;
here's the relevant line of code.

    url_list = [row['url'] for row in dt.execute('SELECT DISTINCT url FROM links WHERE status_code IS NULL')]

I don't remember exactly how this affected the results thought; it might not have
been a big deal.

## Unresponsive datasets
I recorded when HTTP requests for datasets timed out.
(I used the status code -42.)
I checked a few of these manually. Here are two of them.

    dati.trentino.it/storage/f/2013-06-16T111814/_ggeiWE.csv
    https://www-genesis.destatis.de/genesis/online/link/tabelleDownload/46421-0001.html

They both work, but they take a while. Also, I found that they took longer
to download to my desktop computer on my desk than they took to download to
my server in a datacenter.
In case the problem was my internet connection (tethered from a phone),
I used that server in a datacenter to run the checker again on
all datasets that had timed out. Results didn't remarkably change.

## Slow datasets
Those links took a while to download. Maybe my timeout threshold is being hit?

  url = 'http://dati.trentino.it/storage/f/2013-06-16T114537/_EBmYVk.csv'
  import requests

  timeout = requests.head(url, timeout = 2)
  # timeout error

So it is. More detail follows.

  get = requests.get(url)
  head = requests.head(url)

  print(get)
  # <Response [200]>

  print(head)
  # <Response [200]>

  print(head.elapsed)
  # datetime.timedelta(0, 3, 353558)

The link is alive, but my timeout of 2 seconds was too short.
In this case, the request with a timeout failed, so the initial
response took too long. Also, it took a total of 3.35 seconds to
download. The elapsed time isn't the same thing as the time until
which the request will time out, but they're related.

## Bad URLs
A bunch of datasets have a field for an external link but provided
an empty URL.

```{r}
sqldf("
SELECT catalog,count(*)
FROM links
WHERE is_link AND url IS NULL
GROUP BY catalog
ORDER BY 2;
")
```

Then I realized I could look at the errors for each of the links
I checked. I thought I had saved the error in the database, but
my code for doing that turned out to be broken.

I fixed that! Collecting better error information, I ran the checker
on all of the links that had failed before.

### Hostname
I figured that errors might be related to the server that a dataset comes
from, and I figured that the hostname of the URL would be a decent proxy
for server. (In the URL "http://thomaslevine.com/open-data", the hostname
is "thomaslevine.com".) So I wrote a sloppy function to detect these hostnames.

      datasets$hostname <- sub('(?:(?:http|ftp|https)://)?([^/]*)/.*$', '\\1', datasets$url)

Here are the top few hostnames and the number of datasets with each hostname.

```{r}
sort(table(datasets$hostname))[1:10]
```

Having come up with this variable, I now could look at error types by hostname.

### Base error rate

p.errors.total

p.hostname.total

p.hostname.error

p.hostname.facet
### Invalid URLs
The "http:" datasets weren't valid URLs.

```{r}
t.hostnames[['http:']]
```

### Connection errors
Connection errors seem to correspond to some datasets with strange URLs and others for
which the site just can't be contacted.

```{r connectionerror}
errors.connectionerror <- subset(errors, error_type == 'ConnectionError')
errors.connectionerror$has.tab <- grepl('\t', errors.connectionerror$url)
errors.connectionerror$has.space <- grepl(' ', errors.connectionerror$url)
errors.connectionerror$has.backslash <- grepl('[\\]', errors.connectionerror$url)
ggplot(errors.connectionerror) +
  aes(x = has.tab | has.space | has.backslash) +
  geom_bar()
```

### Invalid schemas
Invalid schemas are for datasets sent over protocals other than HTTP, like FTP.

```{r invalidschema}
ggplot(subset(errors, error_type == 'InvalidSchema')) + aes(x = sub(':.*', '', url)) + geom_bar()
```

Most of these schemas indicate that the files are stored on local systems
rather than being accessible from the internet. But a large minority of these
(FTP, specifically) is fully reasonable to put on the internet; I didn't
check them properly.

### Missing schemas
Missing schemas tend to be for datasets where the hostname was not specified.
Examples:

```{r}
t.error_types$MissingSchema$url
```

```{r missingschema}
ggplot(subset(errors, error_type == 'MissingSchema')) + aes(x = grepl('^/', url)) + geom_bar()
```

This is a valid relative URL. I could have gotten the hostname from the site
from which I got the link, but I did not do this.

### SSL Errors
A bunch of sites did not have SSL certificates that I recognized.

```{r}
ggplot(subset(errors, error_type == 'SSLError')) +
  aes(x = hostname) + geom_bar() + coord_flip()
```

I could ignore the certificates and download the dataset, but the SSL warning
is slightly unnerving.

SSL errors explain only a small part of the links I marked as dead. Of the
datasets that I'd marked as dead before, here are the numbers of datasets
with and without SSL errors.

```{r}
table(errors$error_type == 'SSLError')
```

They are interesting, but they don't explain my strange results.

### Timeouts
Once we get rid of the strange URLs, most of these links have no errors or have
timeouts. (Remember, these are the links that I marked as dead in my previous
analysis, and I tried downloading again for the present analysis.)

```{r}
p.hostname.facet
```

The "www-genesis.destatis.de" datasets seem mostly okay, though there are some timeouts.

```{r_plot_destatis}
ggplot(subset(errors, hostname == 'www-genesis.destatis.de')) + aes(x = error_type) + geom_bar()
```

```{r_table_destatis}
t.hostnames[['www-genesis.destatis.de']]
```


p.elapsed

errors.elapsed

The `/storage` endpoint is typically used for the CKAN filestore. It looks like
that can take a while to respond.

p.storage
